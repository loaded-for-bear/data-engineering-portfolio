# 08. CDC（Change Data Capture）— スナップショット差分検出

## 想定シナリオ

- EC企業の商品マスタ（日次5万件）は基幹システムで管理されており、毎日深夜にCSVで全件エクスポートされる
- DWHへの反映を全件洗い替え（TRUNCATE + INSERT）ではなく、差分（INSERT/UPDATE/DELETE）のみ検出して増分ロードする CDC パイプラインを構築する
- 検出した差分は以下にも利用される:
  - 価格変更の監査ログ（コンプライアンス要件）
  - 在庫変動アラート（stock が閾値以下になった場合の通知）

## 制約条件

| 項目 | 値 |
|------|-----|
| 想定データ件数 | 日次 5万件（将来 50万件に拡大予定） |
| SLA | 毎朝 6:00 までにCDC処理完了（基幹システム出力は 5:00） |
| 予算制約 | 月額 3万円以内（クラウドリソース） |
| チーム規模 | エンジニア 3名（自分含む） |
| 処理時間制約 | 5万件で 30秒以内 |

## 使用技術

- Python 3.10
- pandas（CSV読み込み、merge による差分検出、ベクトル演算）
- 標準ライブラリ: pathlib, time

## 設計選定理由（Why this design?）

### CDC方式: スナップショット比較

基幹システムからの出力がCSV全件エクスポートのみであり、ソース側のDB変更権限を持たない。この制約下で DELETE を含む全差分を検出できるのはスナップショット比較のみである。

### 実装: pandas merge + ベクトル演算

- `pd.merge(how="outer", indicator=True)` により、INSERT/DELETE/BOTH を1回の操作で分類できる
- BOTH レコードのカラム比較はベクトル演算（`!=` の Series 比較）で実装し、iterrows を回避。5万件でも数ミリ秒で完了する
- UPDATE 行の changed_columns / details 生成は `apply` を使用。対象が UPDATE 行のみ（全体の数%）のため、パフォーマンス影響は軽微

## 代替案（Alternatives considered）

| CDC方式 | DELETE検出 | ソース変更 | コスト | 不採用理由 |
|---------|-----------|-----------|--------|-----------|
| タイムスタンプベース | 不可 | 不要 | 低 | DELETE を検出できず要件を満たさない。また基幹システムの updated_at が全変更で正しく更新される保証がない |
| ログベース（WAL） | 可能 | 不要 | 高 | 基幹システムのDB権限がなく、Kafka + Debezium の運用コストが月3万円を超える |
| トリガーベース | 可能 | 必要 | 中 | 基幹システムのDDL変更権限がなく、書き込み性能への影響リスクがある |
| **スナップショット比較** | **可能** | **不要** | **低** | **採用。ソース変更不要で全差分を確実に検出。5万件なら全件I/Oのコストも許容範囲** |

## トレードオフ（Trade-offs）

| デメリット | 影響度 | 許容理由 |
|-----------|--------|---------|
| 毎回全件I/O | 中 | 5万件 × 7カラム × 2ファイル ≈ 数十MB。SSD読み込みで1秒未満 |
| メモリに全件保持 | 中 | merge 結果 ≈ 50〜100MB。一般的なサーバー（8GB〜）なら問題なし |
| 変更0件の日でも処理コスト同じ | 低 | 処理時間 0.04秒。日次1回のバッチでありコスト無視可能 |
| 50万件以上でDuckDB等への設計変更が必要 | 中 | 現時点では5万件。拡張時期に合わせて移行計画を立てる |

## 運用設計

### 障害時の再実行方法

1. 監視アラート（メール/Slack）で障害を検知
2. ログから障害箇所を特定（ERROR レベルを確認）
3. **ソースデータの問題**（スナップショット破損・欠損）の場合:
   - 基幹システム側に再エクスポートを依頼し、再出力CSVで再実行
4. **本パイプラインの問題**の場合:
   - 障害報告を起票し、修正をデプロイ
   - 前々日と前日のCSVで処理を実施し、前日の結果と同じか（冪等性）を確認した上で、当日のCSVで再実行

**冪等性**: 本パイプラインは純粋な関数的設計（入力: 2つのCSV → 出力: cdc_report.csv）であり、外部状態を持たない。同じ入力CSVペアで何度実行しても同一の cdc_report.csv が出力される。出力ファイルは毎回上書きのため、再実行で重複が発生しない。

### 再処理戦略

- 過去日の再処理が必要な場合: 該当日のスナップショットCSVペアを指定して再実行
- スナップショットCSVは日付別に保管し、リテンション期間（30日）を設ける

### ログ設計方針

| レベル | 出力内容 | 出力先 |
|--------|---------|--------|
| INFO | 読み込み件数、CDC結果サマリー、処理完了時間 | stdout + ログファイル |
| INFO | 価格変更の監査ログ（UPDATE で price が変更された場合） | 監査ログファイル |
| WARNING | 在庫変動アラート（UPDATE で stock が変更された場合） | アラート通知（Slack/メール） |
| ERROR | ファイル不存在、データ読み込みエラー、予期せぬ例外 | stderr + ログファイル + メール通知 |

## 意思決定ログ（Decision Log）

### 判断1: CDC検出方式の選択

- **迷った点**: 4つのCDC方式のうち、どれを採用するか
- **検討した選択肢**:
  - (A) タイムスタンプベース — updated_at で変更行だけ抽出
  - (B) スナップショット比較 — 2時点のCSVを丸ごと比較
  - (C) ログベース（WAL） — DBログからリアルタイム取得
- **捨てた案の理由**:
  - (A) DELETE を検出できず、要件（削除検出必須）を満たせない
  - (C) 基幹システムのDB権限がなく、Kafka + Debezium のインフラ運用コスト（月5〜10万円想定）が予算3万円を超過
- **採用した案の根拠**: (B) 制約「ソースシステム変更不可」「月3万円以内」を満たす唯一の方式。5万件なら全件比較でも処理時間 0.04秒で、SLA（30秒以内）に対して十分な余裕がある
- **やり直すなら**: 初回からDuckDBを利用する。pandas と同じスナップショット比較方式だが、50万件への拡張時にメモリ問題を回避できる。CSVを直接SQLクエリでき、pip install だけで導入可能なため移行コストも低い

### 判断2: UPDATE/UNCHANGED の判定方法

- **迷った点**: merge 後の BOTH レコードから UPDATE と UNCHANGED をどう判定するか
- **検討した選択肢**:
  - (A) iterrows で1行ずつカラム比較
  - (B) ベクトル演算でカラムごとに一括比較 + any(axis=1)
  - (C) 行全体のハッシュ値で一括判定
- **捨てた案の理由**:
  - (A) 5万件のループは Python レベルで数秒かかる。50万件拡大時に30秒を超えるリスク
  - (C) UPDATE/UNCHANGED の振り分けには有効だが、「どのカラムが変わったか」の特定には使えない。結局 UPDATE 行でカラム比較が必要になり二度手間
- **採用した案の根拠**: (B) ベクトル演算は C レベルで一括処理され、5万件でも数ミリ秒。かつ `_changed` カラムがそのまま changed_columns の特定に使えるため、判定と詳細記録を一貫して処理できる
- **やり直すなら**: 50万件の場合、(C) ハッシュで UNCHANGED を先に除外し、UPDATE 行のみカラム比較する2段階方式も検討する

### 判断3: DataFrame の結合方法

- **迷った点**: INSERT/DELETE/UPDATE/UNCHANGED の4つの DataFrame をどう1つにまとめるか
- **検討した選択肢**:
  - (A) pd.concat で縦結合
  - (B) 元の df_merged に直接ラベル付け
- **捨てた案の理由**: (B) UPDATE のみ changed_columns / details カラムが必要で、INSERT/DELETE とカラム構成が異なるため、直接ラベル付けでは処理が煩雑になる
- **採用した案の根拠**: (A) concat なら異なるカラム構成でも NaN で補完して結合でき、グループごとの加工処理が明確に分離できる。一時的にメモリを2倍使うが、5万件なら100MB以下で問題ない
- **やり直すなら**: メモリ効率を重視するなら (B) + UPDATE のみ後処理。ただしチーム規模3名での保守性を考慮すると (A) が妥当

## 将来拡張案（Scale plan）

### データ量 10倍（50万件）時

- DuckDB を利用し、SQLでスナップショット比較を実施する
- **理由**:
  - 列指向のためカラム単位の比較が高速
  - CSVを直接SQLでクエリでき、DBロード不要
  - メモリに収まらなくてもディスクスピルで処理継続可能
  - pip install duckdb だけで導入でき、月額コスト0円（予算3万円の制約内）
- **推定処理時間**: 現在 0.04秒（5万件）→ 線形スケールで 0.4秒（50万件）。pandas でもSLA内だが、メモリ使用量が500MB〜1GBに達するため DuckDB が安全

### データ量 100倍（500万件）時

- Spark / BigQuery 等の分散処理基盤への移行を検討する
- **理由**: 単一マシンのメモリ（8〜16GB）では merge 結果を保持できない可能性があるため、分散処理が必要
- **推定コスト**: BigQuery の場合、500万件 × 7カラム ≈ 数百MB のスキャンで $0.005/クエリ程度。日次実行で月額 $0.15

## 実行方法

```bash
cd /workspace/03_trial/data_engineering_etl/2026-02-17
python my_code/cdc_snapshot.py
```

### 出力ファイル

- `my_code/cdc_report.csv` — 差分レポート（全17行: INSERT 2, DELETE 1, UPDATE 8, UNCHANGED 6）
